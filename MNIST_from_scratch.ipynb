{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a neural network from scratch to recognize handwritten digits. We will only use nummpy for loading and preprocessing\n",
    "the data. \n",
    "Everything else will be in python, written to be as clear as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 is setting up your imports, and getting and loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching the MNIST dataset...\n",
      "MNIST dataset loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching the MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target\n",
    "print(\"MNIST dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train, validation and test sets\n",
    "#We'll first split into training and testing in a 7:3 ratio\n",
    "#We'll then split the test data into testing and validation in a 2:1 ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Training (X_train and y_train): (49000, 784), (49000,)\n",
      "Validation (X_val and y_val):(6930, 784), (6930,)\n",
      "Test (X_test, y_test): (14070, 784), (14070,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset shapes:\")\n",
    "print(f\"Training (X_train and y_train): {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation (X_val and y_val):{X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test (X_test, y_test): {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's look at a few digits to make sure the data looks right\n",
    "\n",
    "def view_digit(index):\n",
    "    \"\"\"Show a digit along  with the label\"\"\"\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(X_train[index].reshape(28,28), cmap='gray')\n",
    "    plt.title(f\"This is a {y_train[index]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show\n",
    "\n",
    "#Look at a random digit\n",
    "random_index = random.randint(0, len(X_train)-1)\n",
    "view_digit(random_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "This will be the last part where we'll use numpy. Here, we will convert the labels from numbers to vectors:\n",
    "For example, the label 5 would become [0,0,0,0,0,1,0,0,0,0]\n",
    "\n",
    "Why? Well, when we are training a neural network to output the predictions for a random single digit image, you can code the output 3 ways:\n",
    "1. The number itself. You'd want to train the network to output 5 for an image saying 5. This isn't a good idea. Let me illustrate with an example. Let's say you have an image that should be a 4. But the network thinks it's a 5. In which case, the error(how far your prediction is from the actual) would be 1. If the network predicts it to be 9 instead, you've got a larger error, which is 4. This won't do. \"4\" isn't more similar to \"5\" than \"9\" is. They're all just different digits. We want all numbers to be \"equidistant\" from each other - error wise.\n",
    "2. Binary output. This also faces a similar issue. To encode 0 to 9, you'd need 4 binary digits for each number. 6 would be [0,1,1,0] and 7 would be [0,1,1,1] and 8 would be [1,0,0,0]. To go from 7 to 8, you'd have to FLIP all the bits, whereas to go from 6 to 7, you'd only have to flip 1 bit. This again creates a false relationship where 6 is \"closer\" to 7 than 7 is to 8.\n",
    "3. The final option is to have 10 output digits. This is called one hot encoding. 1 would be [0,1,0,0,0,0,0,0,0,0] and 9 would be [[0,0,0,0,0,0,0,0,0,1]]. Every digit is only \"2 flips\" away from any other digit. There are no implied relationships. Error calculations are also easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, num_classes = 10):\n",
    "    #Convert number labels to one hot encoded vectors\n",
    "    encoded = np.zeros((len(labels), num_classes))\n",
    "    for i, label in enumerate(labels):\n",
    "        encoded[i][int(label)] = 1\n",
    "    return encoded\n",
    "\n",
    "#Convert all our labels\n",
    "y_train_encoded = one_hot_encoding(y_train)\n",
    "y_val_encoded = one_hot_encoding(y_val)\n",
    "y_test_encoded = one_hot_encoding(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few labels before and after encoding:\n",
      "Before: ['9' '9' '8']\n",
      "After: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Check if it worked\n",
    "print(\"First few labels before and after encoding:\")\n",
    "print(\"Before:\", y_train[:3])\n",
    "print(\"After:\", y_train_encoded[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3 - No more numpy. All my homies love numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating neural network...\n",
      "\n",
      "Starting training...\n",
      " Epoch 1: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 1: 69.19% accuracy\n",
      " Epoch 2: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 2: 72.15% accuracy\n",
      " Epoch 3: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 3: 74.94% accuracy\n",
      " Epoch 4: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 4: 75.69% accuracy\n",
      " Epoch 5: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 5: 76.44% accuracy\n",
      " Epoch 6: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 6: 73.71% accuracy\n",
      " Epoch 7: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 7: 74.49% accuracy\n",
      " Epoch 8: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 8: 76.26% accuracy\n",
      " Epoch 9: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 9: 75.54% accuracy\n",
      " Epoch 10: Training on 1532 batches\n",
      " Batch 0/1532\n",
      " Batch 100/1532\n",
      " Batch 200/1532\n",
      " Batch 300/1532\n",
      " Batch 400/1532\n",
      " Batch 500/1532\n",
      " Batch 600/1532\n",
      " Batch 700/1532\n",
      " Batch 800/1532\n",
      " Batch 900/1532\n",
      " Batch 1000/1532\n",
      " Batch 1100/1532\n",
      " Batch 1200/1532\n",
      " Batch 1300/1532\n",
      " Batch 1400/1532\n",
      " Batch 1500/1532\n",
      "Epoch 10: 76.35% accuracy\n"
     ]
    }
   ],
   "source": [
    "class My_Neural_Network:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        We are creating a neural network to take an image input and pass a number output.\n",
    "        We know that the image is 28*28 pixels while the output is a 1d array with 10 binary elements.\n",
    "        \n",
    "        Let's do 784 input --> 112 neurons in the first hidden layer --> 16 in the second hidden layer and 10 in the output layer\n",
    "        Then it would be My_Neural_Network[784,112,16,10]\n",
    "        \"\"\"\n",
    "        self.sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "\n",
    "        #Initialize random weights and 1 bias\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        #For each layer (except the input layer), let's create some weights and biases\n",
    "        for i in range(len(self.sizes)-1):\n",
    "            #Create weights b/w the current layer and the next layer\n",
    "            layer_weights = []\n",
    "            for destination_neuron in range(self.sizes[i+1]):\n",
    "                neuron_weights = []\n",
    "                for source_neuron in range(self.sizes[i]):\n",
    "                    #Random weight b/w -0.5 and 0.5\n",
    "                    weight = (random.random() - 0.5)\n",
    "                    neuron_weights.append(weight)\n",
    "                layer_weights.append(neuron_weights)\n",
    "            self.weights.append(layer_weights)\n",
    "        \n",
    "            #Create 1 biases for next layer\n",
    "            layer_biases = [1.0 for _ in range(self.sizes[i+1])]\n",
    "            self.biases.append(layer_biases)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        #This will be our activation function --> for any given input, it will only output a number b/w 0 and 1\n",
    "        if x < -700:\n",
    "            return 0.0\n",
    "        elif x > 700:\n",
    "            return 1.0\n",
    "        \n",
    "        return 1.0/(1.0 + math.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        #This is the derivative of a sigmoid.\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "    \n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\"\n",
    "        Push data through the network to get predictions. It returns:\n",
    "        1. Actvations --> neuron outputs at each layer\n",
    "        2. weighted sums --> sigmoid(weighted_sums) = activations from the previous step (We'll use this for backpropagation)\n",
    "        \"\"\"\n",
    "\n",
    "        #Leep track of values we need for backpropagation\n",
    "        current_values = inputs\n",
    "        all_activations = [inputs] #We'll put this as the input for now. It'll get updated every layer\n",
    "        all_weighted_sums = []\n",
    "\n",
    "        #Go through each layer\n",
    "        for layer_weights, layer_biases in zip(self.weights, self.biases):\n",
    "\n",
    "            #The weighted sum is z = w*a + b\n",
    "            weighted_sums = []\n",
    "            for weights, bias in zip(layer_weights, layer_biases):\n",
    "                total = sum(w * a for w,a in zip(weights, current_values))\n",
    "                weighted_sums.append(total + bias)\n",
    "            all_weighted_sums.append(weighted_sums)\n",
    "        \n",
    "            #Apply the sigmoid function on weighted_sums to get activations\n",
    "            current_values = [self.sigmoid(z) for z in weighted_sums]\n",
    "            all_activations.append(current_values)\n",
    "        \n",
    "        return all_activations, all_weighted_sums\n",
    "\n",
    "    def backpropagate(self, training_example, label):\n",
    "        \"\"\"\n",
    "        1. Find out how much the error was\n",
    "        2. Find out how much each weight and bias contributed to the error\n",
    "        Return weight and bias gradients for each layer.\n",
    "        \"\"\"        \n",
    "\n",
    "        #Forward pass to get all the values we need for backpropagation\n",
    "        activations, weighted_sums = self.feedforward(training_example)\n",
    "\n",
    "        #Initialize gradients as zero\n",
    "        weight_gradients = [[[0.0 for _ in range(len(w_row))]\n",
    "                                for w_row in layer]\n",
    "                                for layer in self.weights]\n",
    "        bias_gradients = [[0.0 for _ in range(len(b))]\n",
    "                            for b in self.biases]\n",
    "        \n",
    "        #Calculate the error at the output layer\n",
    "        output_error = [a - t for a, t in zip(activations[-1], label)]\n",
    "        layer_error = [err * self.sigmoid_derivative(a)\n",
    "                        for err, a in zip(output_error, activations[-1])]\n",
    "        \n",
    "        #Save gradients for output layer\n",
    "        bias_gradients[-1] = layer_error\n",
    "        for i, error in enumerate(layer_error):\n",
    "            for j, activation in enumerate(activations[-2]):\n",
    "                weight_gradients[-1][i][j] = error * activation\n",
    "        \n",
    "        #Backpropagate error through earlier layers\n",
    "        for layer in range(2, self.num_layers):\n",
    "            layer_error_new = []\n",
    "\n",
    "            #For each neuron in current layer\n",
    "            for j in range(len(self.weights[-layer])):\n",
    "                #Calculate error based on the next layer's error.\n",
    "                error = 0.0\n",
    "                for k in range(len(layer_error)):\n",
    "                    error += self.weights[-layer+1][k][j] * layer_error[k]\n",
    "                error *= self.sigmoid_derivative(activations[-layer][j])\n",
    "                layer_error_new.append(error)\n",
    "            \n",
    "            layer_error = layer_error_new\n",
    "\n",
    "            #Save gradients\n",
    "            bias_gradients[-layer] = layer_error\n",
    "            for i, error in enumerate(layer_error):\n",
    "                for j, activation in enumerate(activations[-layer-1]):\n",
    "                    layer_index = len(self.weights) - layer\n",
    "                    weight_gradients[layer_index][i][j] = error * activation\n",
    "    \n",
    "        return weight_gradients, bias_gradients\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, learning_rate):\n",
    "        #Update network weights using a small batch of shuffled training data\n",
    "\n",
    "        #Initialize gradient sums as zero\n",
    "\n",
    "        weight_sums = [[[0.0 for _ in range(len(w_row))]\n",
    "                        for w_row in layer]\n",
    "                        for layer in self.weights]\n",
    "        \n",
    "        bias_sums = [[0.0 for _ in range(len(b))]\n",
    "                        for b in self.biases]\n",
    "        \n",
    "        #Calculate gradients for each training example\n",
    "        for inputs, label in mini_batch:\n",
    "            weight_gradients, bias_gradients = self.backpropagate(inputs, label)\n",
    "\n",
    "            #Add to sums\n",
    "            for layer in range(len(weight_sums)):\n",
    "                for i in range(len(weight_sums[layer])):\n",
    "                    for j in range(len(weight_sums[layer][i])):\n",
    "                        weight_sums[layer][i][j] += weight_gradients[layer][i][j]\n",
    "            \n",
    "            for layer in range(len(bias_sums)):\n",
    "                for i in range(len(bias_sums[layer])):\n",
    "                    bias_sums[layer][i] += bias_gradients[layer][i]\n",
    "        \n",
    "        # Update weights and biases using average gradients\n",
    "        batch_size = len(mini_batch)\n",
    "        for layer in range(len(self.weights)):\n",
    "            for i in range(len(self.weights[layer])):\n",
    "                for j in range(len(self.weights[layer][i])):\n",
    "                    self.weights[layer][i][j] -= (learning_rate/batch_size) * weight_sums[layer][i][j]\n",
    "            \n",
    "            for i in range(len(self.biases[layer])):\n",
    "                self.biases[layer][i] -= (learning_rate/batch_size) * bias_sums[layer][i]\n",
    "    \n",
    "    def train(self, training_data, epochs, mini_batch_size, learning_rate):\n",
    "        #Train the network on batches of data\n",
    "        n = len(training_data)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            #Shuffle the training data\n",
    "            random.shuffle(training_data)\n",
    "\n",
    "            #Create mini-batches\n",
    "            mini_batches = [\n",
    "                training_data[k:k+ mini_batch_size]\n",
    "                for k in range(0, n , mini_batch_size)\n",
    "            ]\n",
    "\n",
    "            #Train on each mini-batch\n",
    "            print(f\" Epoch {epoch + 1}: Training on {len(mini_batches)} batches\")\n",
    "            for i, mini_batch in enumerate(mini_batches):\n",
    "                if i % 100 == 0:\n",
    "                    #Show progress every 100 batches\n",
    "                    print(f\" Batch {i}/{len(mini_batches)}\")\n",
    "                self.update_mini_batch(mini_batch, learning_rate)\n",
    "            \n",
    "            #Calculate and show accuracy\n",
    "            accuracy = self.evaluate(validation_data)\n",
    "            print(f\"Epoch {epoch+1}: {accuracy:.2f}% accuracy\")\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        #Get network's prediction (0-9) for an input\n",
    "        activations, _ = self.feedforward(inputs)\n",
    "        output = activations[-1]\n",
    "        return output.index(max(output))\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        #Calcuate accuracy on test data\n",
    "        correct = 0\n",
    "        total = len(test_data)\n",
    "\n",
    "        for inputs, label in test_data:\n",
    "            prediction = self.predict(inputs)\n",
    "            actual = list(label).index(1) \n",
    "            #We are bringing the label back from one-hot-encoded to a numeric form\n",
    "            if prediction == actual:\n",
    "                correct += 1\n",
    "        \n",
    "        return (correct/total)*100 \n",
    "            \n",
    "#And that's it. 200 lines of code for a neural network in python\n",
    "\n",
    "# First, convert numpy arrays to lists and normalize\n",
    "X_train_list = [list(x/255.0) for x in X_train]\n",
    "X_val_list = [list(x/255.0) for x in X_val]\n",
    "X_test_list = [list(x/255.0) for x in X_test]\n",
    "\n",
    "# Create training data pairs\n",
    "training_data = list(zip(X_train_list, y_train_encoded))\n",
    "validation_data = list(zip(X_val_list, y_val_encoded))\n",
    "test_data = list(zip(X_test_list, y_test_encoded))\n",
    "\n",
    "# Create and train network\n",
    "print(\"\\nCreating neural network...\")\n",
    "network = My_Neural_Network([784, 112, 16, 10])\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "network.train(\n",
    "    training_data,\n",
    "    epochs=10,\n",
    "    mini_batch_size=32,\n",
    "    learning_rate=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: This is an amazing resource that really helped me understand backprop in detail -- https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def export_weights():\n",
    "    # Convert nested lists to format suitable for JSON\n",
    "    weights_json = []\n",
    "    for layer in network.weights:\n",
    "        layer_weights = []\n",
    "        for neuron in layer:\n",
    "            layer_weights.append(neuron)\n",
    "        weights_json.append(layer_weights)\n",
    "    \n",
    "    biases_json = []\n",
    "    for layer in network.biases:\n",
    "        layer_biases = []\n",
    "        for bias in layer:\n",
    "            layer_biases.append(float(bias))  # Convert to float for JSON serialization\n",
    "        biases_json.append(layer_biases)\n",
    "    \n",
    "    model_data = {\n",
    "        'weights': weights_json,\n",
    "        'biases': biases_json\n",
    "    }\n",
    "    \n",
    "    with open('mnist_weights.json', 'w') as f:\n",
    "        json.dump(model_data, f)\n",
    "\n",
    "# Call this after training\n",
    "export_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final test set accuracy: 76.34%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = network.evaluate(test_data)\n",
    "print(f\"\\nFinal test set accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
